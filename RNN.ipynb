{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb667b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install scikit-learn\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import io\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenization from corpus file\n",
    "def tokenize():\n",
    "    with open(\"import_export_knowledge.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        corpus = file.read()\n",
    "    sentence_tokens = nltk.sent_tokenize(corpus)\n",
    "    word_tokens = nltk.word_tokenize(corpus)\n",
    "    return sentence_tokens, word_tokens\n",
    "\n",
    "# Preprocessing\n",
    "def lemtokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "punct = dict((ord(i), None) for i in string.punctuation)\n",
    "\n",
    "def lemmer(text):\n",
    "    return lemtokens(nltk.word_tokenize(text.lower().translate(punct)))\n",
    "\n",
    "# Greeting detection\n",
    "greeting_inputs = ['hello', 'hi', 'hey', 'greetings']\n",
    "greeting_responses = [\n",
    "    'Hello, I am a chatbot. My name is Alan.',\n",
    "    'Hello!',\n",
    "    'Hi!',\n",
    "    'Wassup!',\n",
    "    'Hi there!'\n",
    "]\n",
    "\n",
    "def greeting(text):\n",
    "    for token in text.split():\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)\n",
    "\n",
    "# Response generation\n",
    "def respond(user_query):\n",
    "    bot_response = ''\n",
    "    sent_tokens, word_tokens = tokenize()\n",
    "    sent_tokens.append(user_query)\n",
    "\n",
    "    tfidf_obj = TfidfVectorizer(tokenizer=lemmer, stop_words='english')\n",
    "    tfidf = tfidf_obj.fit_transform(sent_tokens)\n",
    "\n",
    "    sim_values = cosine_similarity(tfidf[-1], tfidf)\n",
    "    index = sim_values.argsort()[0][-2]\n",
    "    flat = sim_values.flatten()\n",
    "    flat.sort()\n",
    "    required_tfidf = flat[-2]\n",
    "\n",
    "    if required_tfidf == 0:\n",
    "        bot_response += 'I cannot understand'\n",
    "    else:\n",
    "        bot_response += sent_tokens[index]\n",
    "\n",
    "    return bot_response\n",
    "\n",
    "# Main chatbot loop\n",
    "print(\"Alan (Import/Export Chatbot)\")\n",
    "flag = 1\n",
    "while flag:\n",
    "    user_query = input(\"You: \").lower()\n",
    "    if user_query == 'exit':\n",
    "        flag = 0\n",
    "        print(\"Alan: Bye, see you!\")\n",
    "    elif greeting(user_query) is not None:\n",
    "        print(\"Alan:\", greeting(user_query))\n",
    "    else:\n",
    "        print(\"Alan:\", respond(user_query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
